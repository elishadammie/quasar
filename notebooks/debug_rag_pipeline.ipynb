{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20d8419-9889-42d3-ab81-df00a9d80616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables...\n",
      "Environment variables loaded.\n",
      "Creating insecure httpx client...\n",
      "Client created.\n",
      "Initializing LLM and Embeddings clients...\n",
      "Clients initialized.\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "print(\"Loading environment variables...\")\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "print(\"Environment variables loaded.\")\n",
    "\n",
    "# --- SSL FIX FOR CORPORATE PROXY ---\n",
    "print(\"Creating insecure httpx client...\")\n",
    "insecure_client = httpx.Client(verify=False)\n",
    "print(\"Client created.\")\n",
    "\n",
    "# --- Initialize Clients ---\n",
    "print(\"Initializing LLM and Embeddings clients...\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=OPENAI_API_KEY, http_client=insecure_client)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_key=OPENAI_API_KEY, http_client=insecure_client)\n",
    "print(\"Clients initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44912ed9-105f-4ac5-a277-99de4bd0a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question set to: 'What is the main purpose of the system described in the document?'\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the main purpose of the system described in the document?\"\n",
    "print(f\"Question set to: '{question}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dfa2e34-9c04-495f-afdc-cb22191d9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PGVector store...\n",
      "Creating retriever...\n",
      "Retrieving context for the question: 'What is the main purpose of the system described in the document?'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2162/3580055909.py:4: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details about the new implementation.\n",
      "  vector_store = PGVector(\n",
      "/tmp/ipykernel_2162/3580055909.py:4: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  vector_store = PGVector(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SUCCESS: CONTEXT RETRIEVED ---\n",
      "Retrieved 4 document chunks.\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"quasar_doc_collection\"\n",
    "\n",
    "print(\"Initializing PGVector store...\")\n",
    "vector_store = PGVector(\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    ")\n",
    "\n",
    "print(\"Creating retriever...\")\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "print(f\"Retrieving context for the question: '{question}'...\")\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "print(\"\\n--- SUCCESS: CONTEXT RETRIEVED ---\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} document chunks.\")\n",
    "# Optional: print the retrieved context to see what the agent found\n",
    "# for i, doc in enumerate(retrieved_docs):\n",
    "#     print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1e3266-3cfc-4d00-b99c-99872d127b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERATING ANSWER (This is the test) ---\n",
      "Sending request to LLM... Please wait...\n",
      "\n",
      "--- SUCCESS: ANSWER GENERATED! ---\n",
      "Final Answer: The main purpose of the system described in the document is the development and validation of a computational model for early Alzheimer's Disease and Related Disorders (ADRD) diagnosis.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"--- GENERATING ANSWER (This is the test) ---\")\n",
    "print(\"Sending request to LLM... Please wait...\")\n",
    "\n",
    "# This is the line that is likely hanging\n",
    "final_answer = rag_chain.invoke({\"question\": question, \"context\": retrieved_docs})\n",
    "\n",
    "print(\"\\n--- SUCCESS: ANSWER GENERATED! ---\")\n",
    "print(\"Final Answer:\", final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e403ff-4b61-4224-802f-1ce46cc773d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
